[general]
arch = armv8 # !
enable_icache_modeling = true
mode = 64 # !
syntax = default
#total_cores = 4 # !

[perf_model/core]
core_model = cortex-a53 # !
frequency = 1.51 # !
type = rob # !
logical_cpus = 1 # Number of SMT threads per core !

[perf_model/core/rob_timer]
address_disambiguation = false  # Allow loads to bypass preceding stores with an unknown address
commit_width = 2                      # Commit bandwidth (instructions per cycle), per SMT thread ?
in_order = true # !
issue_contention = true
issue_memops_at_issue = true  # Issue memops to the memory hierarchy at issue time (false = before dispatch)
mlp_histogram = false
outstanding_loads = 8 #?
outstanding_stores = 16 #?
rob_repartition = true
rs_entries = 36
simultaneous_issue = true
store_to_load_forwarding = false

[perf_model/core/interval_timer]
dispatch_width = 2
window_size = 128
num_outstanding_loadstores = 10

[perf_model/branch_predictor]
type = a53
mispredict_penalty = 10 # ?
size = 1024  # 2.1.1 of the technical reference manual ?
num_history_registers = 3

[perf_model/cache]
levels = 2 # !

[perf_model/tlb]
penalty = 19   # "variable number of cycles"

[perf_model/itlb]
size = 10 # Number of I-TLB entries !
associativity = 10 # full associative !

[perf_model/dtlb]
size = 10 # Number of D-TLB entries !
associativity = 10 # fully associative !

[perf_model/stlb]
size = 512 # Number of second-level TLB entries !
associativity = 4 # S-TLB associativity !

[perf_model/l1_icache]
cache_block_size = 64 # !
cache_size = 32 # ! 
associativity = 2 # !
replacement_policy = random # !
writethrough = 0
perfect = false
passthrough = false
coherent = true
data_access_time = 2
tags_access_time = 1
perf_model_type = parallel
writeback_time = 0    # Extra time required to write back data to a higher cache level
dvfs_domain = core    # Clock domain: core or global
shared_cores = 1      # Number of cores sharing this cache
next_level_read_bandwidth = 128 # Read bandwidth to next-level cache, in bits/cycle, 0 = infinite
prefetcher = none
address_hash = mask

[perf_model/l1_dcache]
cache_block_size = 64 # !
cache_size = 32 # in KB !
associativity = 4 # !
replacement_policy = random # !
prefetcher = a53prefetcher  #??
writethrough = 0 # !
perfect = false
passthrough = false
address_hash = mask
data_access_time = 1
tags_access_time = 1
perf_model_type = parallel
writeback_time = 0    # Extra time required to write back data to a higher cache level
dvfs_domain = core    # Clock domain: core or global
shared_cores = 1      # Number of cores sharing this cache
outstanding_misses = 3 # !
next_level_read_bandwidth = 128 # Read bandwidth to next-level cache, in bits/cycle, 0 = infinite

[perf_model/l1_dcache/prefetcher]
prefetch_on_prefetch_hit = false
train_prefetcher_on_hit = false
delay_prefetcher = false

[perf_model/l1_dcache/prefetcher/a53prefetcher]
num_prefetches=3
pattern_length=100
consecutive_pattern_length=2

[perf_model/l1_dcache/prefetcher/simple]
flows = 8
flows_per_core = false
num_prefetches = 32
stop_at_page_boundary = true

[perf_model/l2_cache]
cache_block_size = 64 # in bytes !
cache_size = 512 # in KB
associativity = 16 # !
replacement_policy = random # it cannot be found in the technical reference manual, so I will just asume that it's as the l1 cache
data_access_time = 25 # Maximum from the overall RAM latency calculation table, tech ref manual
tags_access_time = 5 # Maximum from the overall RAM latency calculation table, tech ref manual
writeback_time = 0 # approx of 1.5 ns; 3 cycles at 2 GHz; Extra time required to write back data to a higher cache level
prefetcher = none # optional
next_level_read_bandwidth = 0 # 2 cycles at 2 GHz; Read bandwidth to next-level cache, in bits/cycle, 0 = infinite
writethrough = 0 # !
perfect = false
passthrough = false
address_hash = mask
dvfs_domain = core    # Clock domain: core or global
shared_cores = 1      # Number of cores sharing this cache
outstanding_misses = 0
perf_model_type = parallel

[perf_model/l2_cache/prefetcher]
prefetch_on_prefetch_hit = false

[perf_model/l2_cache/prefetcher/simple]
flows = 64
flows_per_core = true
num_prefetches = 8
stop_at_page_boundary = true

[perf_model/llc]
evict_buffers = 0 # minimum from the manual: "Configurable number of Fill/Eviction Queue (FEQ) entries to 20, 24, or 28." ?

[caching_protocol]
type = parametric_dram_directory_msi
variant = mesi # moesi

[perf_model/dram_directory]
total_entries = 16384
associativity = 16
directory_type = full_map # Supported (full_map, limited_no_broadcast, limitless)

[perf_model/dram]
type = constant # DRAM performance model type: "constant" or a "normal" distribution
latency = 180 # In nanoseconds !
per_controller_bandwidth = 5 # In GB/s
num_controllers = -1 # Total Bandwidth = per_controller_bandwidth * num_controllers
controllers_interleaving = 0 # If num_controllers == -1, place a DRAM controller every N cores
chips_per_dimm = 8
dimms_per_controller = 4

[perf_model/sync]
reschedule_cost = 0 # In nanoseconds

[network/bus]
ignore_local_traffic = true # Do not count traffic between core and directory on the same tile
bandwidth = 5.5 # !

[dvfs]
type = simple
transition_latency = 0 # In nanoseconds
